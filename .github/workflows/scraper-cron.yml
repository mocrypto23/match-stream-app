name: Scrape matches (15m)

on:
  schedule:
    - cron: "*/15 * * * *"
  workflow_dispatch:

concurrency:
  group: scrape-matches
  cancel-in-progress: true

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # ✅ NEW: إثبات 100% أي Commit وأي scraper.js بيتنفذ
      - name: Show commit + scraper fingerprint
        run: |
          echo "================ RUN INFO ================"
          echo "GITHUB_SHA: $GITHUB_SHA"
          echo "GITHUB_REF: $GITHUB_REF"
          echo "GITHUB_REF_NAME: $GITHUB_REF_NAME"
          echo "=========================================="
          echo "git rev-parse HEAD:"
          git rev-parse HEAD
          echo "git show -s --oneline:"
          git show -s --oneline
          echo "=========================================="
          echo "scraper.js sha256:"
          sha256sum scraper.js || true
          echo "=========================================="
          echo "Markers inside scraper.js:"
          grep -n "FINAL FIXES" scraper.js || true
          grep -n "now-30m" scraper.js || true
          grep -n "function parseScore" -n scraper.js || true
          grep -n "\\^\\\\d\\{1,2\\}\\$" scraper.js || true
          echo "=========================================="
          echo "parseScore area (lines 1..220):"
          nl -ba scraper.js | sed -n '1,220p'

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: "npm"

      - name: Install dependencies
        run: npm ci

      - name: Cache Playwright browsers
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-playwright-${{ hashFiles('package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-playwright-

      - name: Install Playwright (Chromium)
        run: npx playwright install --with-deps chromium

      - name: Install Xvfb
        run: sudo apt-get update && sudo apt-get install -y xvfb

      - name: Run scraper (Xvfb headed)
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          RPC_NAME: refresh_match_stream_app
          CONCURRENCY: 2
          HEADLESS: 0
          DEBUG: 0
          DIAG: 1
        run: xvfb-run -a node scraper.js

      - name: Diagnose Supabase rows (today)
        if: always()
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          TABLE_NAME: match-stream-app
        run: node scripts/diag-supabase.js

      - name: Probe site rendering (runner)
        if: always()
        env:
          HEADLESS: 0
        run: xvfb-run -a node scripts/diag-site.js

      - name: List diagnostics files
        if: always()
        run: |
          echo "=== diag tree ==="
          ls -la
          ls -la diag || true
          find diag -type f -maxdepth 3 -print || true

      - name: Upload diagnostics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: diag
          path: diag
          if-no-files-found: warn

      # ✅ NEW: ارفع نسخة scraper.js اللي اتنفذت فعلاً (علشان نقفل أي جدال)
      - name: Upload executed scraper.js
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: executed-scraper-js
          path: scraper.js
          if-no-files-found: warn

      - name: Upload debug.png if failed
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: debug
          path: debug.png
          if-no-files-found: ignore
