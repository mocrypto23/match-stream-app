name: Scrape matches (15m)

on:
  schedule:
    - cron: "*/15 * * * *"
  workflow_dispatch:

concurrency:
  group: scrape-matches
  cancel-in-progress: true

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: "npm"

      - name: Install dependencies
        run: npm ci

      - name: Cache Playwright browsers
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-playwright-${{ hashFiles('package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-playwright-

      - name: Install Playwright (Chromium)
        run: npx playwright install --with-deps chromium

      - name: Run scraper
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          RPC_NAME: refresh_match_stream_app
          CONCURRENCY: 2
          HEADLESS: 1
          DEBUG: 0
          DIAG: 1
        run: node scraper.js

      # Upload diagnostics even on success (this is what tells us why GitHub differs)
      - name: Upload diagnostics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: diag
          path: diag
          if-no-files-found: ignore

      - name: Upload debug.png if failed
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: debug
          path: debug.png
          if-no-files-found: ignore
